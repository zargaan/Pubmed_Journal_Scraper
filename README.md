# PubMed Journal Scraper ğŸ”¥ğŸ“š

[![Scrapy](https://img.shields.io/badge/Scrapy-2.11%2B-red)](https://scrapy.org/)
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://python.org)

Proyek ini bertujuan untuk mengumpulkan data judul Artikel dari situs web PubMed dengan teknik menggunakan teknik scrapy dan manajemen halaman otomatis.

**Nama Kelompok :**
1. Agato Uria Oidamar Prawira
2. Ndaniel Mahulae
3. Aracel Nestova Aprilyanto

## ğŸ“Œ Daftar Isi
- [Sumber_Data](#-sumber_data)
- [Struktur Direktori](#-struktur-direktori-pubmed_journal_scraper)
- [Fitur](#-fitur)
- [Instalasi](#-instalasi)
- [Penggunaan](#-penggunaan)
- [Konfigurasi](#-konfigurasi)
- [Output](#-output)
- [Troubleshooting](#-troubleshooting)
- [Etika](#-etika)
- [Lisensi](#-lisensi)

---

## Sumber_data
Website : [PubMed](https://pubmed.ncbi.nlm.nih.gov/)

---

## Struktur-direktori-pubmed_journal_scraper

```plaintext
ğŸ“¦.scrapy
 â”— ğŸ“‚httpcache
 â”ƒ â”— ğŸ“‚pubmed
 â”ƒ â”ƒ â”£ ğŸ“‚13
 â”ƒ â”ƒ â”ƒ â”— ğŸ“‚13a154ffd71c93895a679f8f50ea428c140958af
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œmeta
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œpickled_meta
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œrequest_body
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œrequest_headers
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œresponse_body
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œresponse_headers
 â”ƒ â”ƒ â”£ ğŸ“‚26
 â”ƒ â”ƒ â”ƒ â”— ğŸ“‚26bfa22f4171e6f75c0561e84ccc0b840e50188d
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œmeta
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œpickled_meta
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œrequest_body
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œrequest_headers
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œresponse_body
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œresponse_headers
 â”ƒ â”ƒ â”£ ğŸ“‚30
 â”ƒ â”ƒ â”ƒ â”— ğŸ“‚30834863ccc621103b7ac34a8827fa456d99564c
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œmeta
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œpickled_meta
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œrequest_body
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œrequest_headers
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œresponse_body
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œresponse_headers
 â”ƒ â”ƒ â”£ ğŸ“‚46
 â”ƒ â”ƒ â”ƒ â”— ğŸ“‚46c368416c4e727a076a7b465a9c4394247006ec
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œmeta
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œpickled_meta
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œrequest_body
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œrequest_headers
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œresponse_body
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œresponse_headers
 â”ƒ â”ƒ â”£ ğŸ“‚52
 â”ƒ â”ƒ â”ƒ â”— ğŸ“‚522cb827f0b18deb30c7b40ff9786edda1aa2552
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œmeta
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œpickled_meta
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œrequest_body
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œrequest_headers
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œresponse_body
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œresponse_headers
 â”ƒ â”ƒ â”— ğŸ“‚e8
 â”ƒ â”ƒ â”ƒ â”— ğŸ“‚e8fcbc4cf8a8a53f568674cb09615f94b1df8f42
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œmeta
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œpickled_meta
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œrequest_body
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œrequest_headers
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œresponse_body
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œresponse_headers
ğŸ“¦journal_scraper
 â”£ ğŸ“‚spiders
 â”ƒ â”£ ğŸ“‚__pycache__
 â”ƒ â”ƒ â”£ ğŸ“œpubmed.cpython-313.pyc
 â”ƒ â”ƒ â”— ğŸ“œ__init__.cpython-313.pyc
 â”ƒ â”£ ğŸ“œpubmed.py
 â”ƒ â”— ğŸ“œ__init__.py
 â”£ ğŸ“‚__pycache__
 â”ƒ â”£ ğŸ“œpipelines.cpython-313.pyc
 â”ƒ â”£ ğŸ“œsettings.cpython-313.pyc
 â”ƒ â”— ğŸ“œ__init__.cpython-313.pyc
 â”£ ğŸ“œitems.py
 â”£ ğŸ“œmiddlewares.py
 â”£ ğŸ“œpipelines.py
 â”£ ğŸ“œsettings.py
 â”— ğŸ“œ__init__.py
 ğŸ“¦.gitattributes
 ğŸ“¦ai_hasil.csv
 ğŸ“¦cysec_hasil.csv
 ğŸ“¦is_hasil.csv
 ğŸ“¦ml_hasil.csv
 ğŸ“¦scrapy.cfg
```

---

## ğŸš€ Fitur
- **Auto-pagination** hingga 1000 halaman
- **Sistem anti-blokir:**
  - Delay acak (3-10 detik) â±ï¸
  - Rotasi header request ğŸŒ€
  - Retry otomatis saat error â™»ï¸
- **Ekstraksi data:**
  - Judul + abstrak ğŸ“‘
  - Daftar penulis + afiliasi ğŸ‘¨ğŸ’»
  - Info jurnal + tahun publikasi ğŸ“…
  - Link artikel + PMID ğŸ”—

---

## âš™ï¸ Instalasi

### 1ï¸âƒ£ Clone Repo
```bash
git clone https://github.com/username/pubmed-scraper.git
cd pubmed-scraper
```

### 2ï¸âƒ£ Setup Virtual Environment
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate  # Windows
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

---

## ğŸ› ï¸ Penggunaan

### Scrape Default (5 Halaman)
```bash
scrapy crawl pubmed -o hasil.json
```

### Scrape dengan Parameter Kustom
```bash
# Scrape 10 halaman dengan delay 5 detik
scrapy crawl pubmed -s MAX_PAGES=10 -s DOWNLOAD_DELAY=5 -o hasil.json
```

### Modifikasi Kata Kunci
Edit `pubmed_spider.py`:
```python
start_urls = ['https://pubmed.ncbi.nlm.nih.gov/?term=your_keyword_here']
```

---

## âš¡ Konfigurasi
| Parameter        | Deskripsi                     | Default | Contoh             |
|-----------------|-----------------------------|---------|--------------------|
| `MAX_PAGES`     | Maksimum halaman             | 5       | `-s MAX_PAGES=20`  |
| `DOWNLOAD_DELAY`| Jeda antar request (detik)   | 3       | `-s DELAY=10`      |
| `RETRY_TIMES`   | Percobaan ulang saat gagal  | 2       | `-s RETRY=5`       |

---

## ğŸ“„ Contoh Output
```json
{
  "title": "AI in Cancer Diagnosis: A Systematic Review",
  "authors": "Zhang L, Smith J, Lee K, et al",
  "journal": {
    "name": "Nature Medicine",
    "year": 2023,
    "citation": "Nat Med. 2023 Jan;29(1):45-53. doi: 10.1038/s41591-022-02145-y."
  },
  "pmid": "36624315",
  "page": 7,
  "url": "https://pubmed.ncbi.nlm.nih.gov/36624315/"
}
```

---

## ğŸš¨ Troubleshooting

### âŒ Problem: Artikel Tidak Terekstrak
âœ… **Solusi:**
```python
# Cek selector terbaru
documents = response.css('.docsum-content').getall()
print(documents)
```

### âŒ Problem: Paginasi Error
âœ… **Debug URL:**
```bash
scrapy shell 'https://pubmed.ncbi.nlm.nih.gov/?term=test'
```
Di shell:
```python
fetch(response.url)
print(f"Page: {response.meta['page_number']}")
```

### âŒ Problem: IP Terblokir
âœ… **Mitigasi:**
Tambah proxy di `settings.py`:
```python
ROTATING_PROXY_LIST = [
    'http://proxy1:port',
    'http://proxy2:port'
]
```

---

## âš ï¸ Etika
- **Patuhi** `robots.txt` PubMed
- **Cache hasil secara lokal** untuk mengurangi request
- **Hindari scraping data pribadi**

---

## ğŸ“œ Lisensi
Proyek ini dilisensikan di bawah [MIT License](LICENSE).
